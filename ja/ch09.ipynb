{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "1HhY1_KNjX_Y",
        "tags": []
      },
      "source": [
        "# 第9章: 事前学習済み言語モデル（BERT型）\n",
        "\n",
        "本章では、BERT型の事前学習済みモデルを利用して、マスク単語の予測や文ベクトルの計算、評判分析器（ポジネガ分類器）の構築に取り組む。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "AYGvC9HJEVUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570aa129-5694-45dd-edce-b8fe3af5b555"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import BertModel, BertForMaskedLM, BertTokenizer"
      ],
      "metadata": {
        "id": "7qQqR5rwAS_m"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Ue3lp3KJFrwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fddbda7-23ef-409b-aeac-f1816655c15b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer info\n",
        "print(tokenizer.vocab_size)\n",
        "print(tokenizer.model_max_length)\n",
        "print(tokenizer.model_input_names)\n",
        "print(tokenizer.cls_token)\n",
        "print(tokenizer.sep_token)\n",
        "print(tokenizer.pad_token)\n",
        "print(tokenizer.unk_token)\n",
        "print(tokenizer.mask_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hTZTCqnCj-M",
        "outputId": "18b97c5a-0f13-4ade-88ec-af6ad16d7657"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30522\n",
            "512\n",
            "['input_ids', 'token_type_ids', 'attention_mask']\n",
            "[CLS]\n",
            "[SEP]\n",
            "[PAD]\n",
            "[UNK]\n",
            "[MASK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyLKl_eo_80z"
      },
      "source": [
        "## 80. トークン化\n",
        "\n",
        "\"The movie was full of incomprehensibilities.\"という文をトークンに分解し、トークン列を表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_80 = \"The movie was full of incomprehensibilities.\"\n",
        "\n",
        "tokenized_text_80 = tokenizer(text_80)\n",
        "print(tokenized_text_80)"
      ],
      "metadata": {
        "id": "5EYNAvRiEcS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14b597f-8bf7-4d24-ec37-fd670aa973ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1996, 3185, 2001, 2440, 1997, 4297, 25377, 2890, 10222, 5332, 14680, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lbx12UDACt6"
      },
      "source": [
        "## 81. マスクの予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"を埋めるのに最も適切なトークンを求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "for token in special_tokens:\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"{token}: {token_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsLoe3cBENYS",
        "outputId": "ac7ac283-c686-4ed8-f1eb-414a25067981"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD]: 0\n",
            "[UNK]: 100\n",
            "[CLS]: 101\n",
            "[SEP]: 102\n",
            "[MASK]: 103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_81 = \"The movie was full of [MASK].\"\n",
        "\n",
        "inputs = tokenizer(text_81, return_tensors=\"pt\")\n",
        "\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "print(inputs)\n",
        "print(mask_token_index)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    print(outputs.logits.shape)#torch.Size([1, 9, 30522]) バッチサイズ　トークン数　語彙\n",
        "    logits = outputs.logits\n",
        "\n",
        "mask_logits = logits[0, mask_token_index, :]\n",
        "print(mask_logits.shape)\n",
        "\n",
        "top_token_id = torch.argmax(mask_logits, dim=1) #各行における最大値のインデックスを返すからdim=1ってことかな？\n",
        "print(f\"top_token_id:{top_token_id}\")\n",
        "\n",
        "predicted_token = tokenizer.convert_ids_to_tokens(top_token_id)\n",
        "\n",
        "print(f\"Predicted token: {predicted_token}\")\n"
      ],
      "metadata": {
        "id": "KDCV3mRGGe9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16aa878-2161-4260-ddd4-c90471a62a34"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 1996, 3185, 2001, 2440, 1997,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([6])\n",
            "torch.Size([1, 9, 30522])\n",
            "torch.Size([1, 30522])\n",
            "top_token_id:tensor([4569])\n",
            "Predicted token: ['fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39DStXDk3OG"
      },
      "source": [
        "## 82. マスクのtop-k予測\n",
        "\n",
        "\"The movie was full of [MASK].\"の\"[MASK]\"に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_82 = \"The movie was full of [MASK].\"\n",
        "\n",
        "inputs = tokenizer(text_82, return_tensors=\"pt\")\n",
        "\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    print(outputs.logits.shape)#torch.Size([1, 9, 30522]) バッチサイズ　トークン数　語彙\n",
        "    logits = outputs.logits\n",
        "\n",
        "mask_logits = logits[0, mask_token_index[0], :] #shape: [vocab_size]\n",
        "\n",
        "mask_logits_prob = F.softmax(mask_logits, dim=0)\n",
        "print(mask_logits.shape)\n",
        "print(mask_logits)\n",
        "print(mask_logits_prob.shape)\n",
        "print(mask_logits_prob)\n",
        "\n",
        "topk = torch.topk(mask_logits_prob, k=10)\n",
        "\n",
        "for idx, score in zip(topk.indices, topk.values):\n",
        "  token = tokenizer.convert_ids_to_tokens(idx.item())\n",
        "  print(f\"{token}: {score.item():.2f}\")"
      ],
      "metadata": {
        "id": "EH0D0BTNgubL",
        "outputId": "d1270f8f-a6af-4191-cc38-26cf776f46ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 9, 30522])\n",
            "torch.Size([30522])\n",
            "tensor([-3.6503, -3.4497, -3.2653,  ..., -3.2588, -2.6857, -4.3190])\n",
            "torch.Size([30522])\n",
            "tensor([2.5729e-07, 3.1443e-07, 3.7810e-07,  ..., 3.8057e-07, 6.7506e-07,\n",
            "        1.3182e-07])\n",
            "fun: 0.11\n",
            "surprises: 0.07\n",
            "drama: 0.04\n",
            "stars: 0.03\n",
            "laughs: 0.03\n",
            "action: 0.02\n",
            "excitement: 0.02\n",
            "people: 0.02\n",
            "tension: 0.02\n",
            "music: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zr6VVYiRPzk"
      },
      "source": [
        "## 83. CLSトークンによる文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "sentences = [\"The movie was full of fun.\",\n",
        "             \"The movie was full of excitement.\",\n",
        "             \"The movie was full of crap.\",\n",
        "             \"The movie was full of rubbish.\"]\n",
        "\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs"
      ],
      "metadata": {
        "id": "qm-_RsOGlx28",
        "outputId": "47fd4e2f-3eca-49c4-e1b7-10a6679af47f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1996,  3185,  2001,  2440,  1997,  4569,  1012,   102],\n",
              "        [  101,  1996,  3185,  2001,  2440,  1997,  8277,  1012,   102],\n",
              "        [  101,  1996,  3185,  2001,  2440,  1997, 10231,  1012,   102],\n",
              "        [  101,  1996,  3185,  2001,  2440,  1997, 29132,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqo5Ufzkyc89"
      },
      "source": [
        "## 84. 平均による文ベクトル\n",
        "\n",
        "以下の文の全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
        "\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s934DWT_1kFm"
      },
      "source": [
        "## 85. データセットの準備\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) から訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiXmLuWpD3_Q"
      },
      "source": [
        "## 86. ミニバッチの作成\n",
        "\n",
        "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4nrV6bqD-m9"
      },
      "source": [
        "## 87. ファインチューニング\n",
        "\n",
        "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqDN0G2UqFJ"
      },
      "source": [
        "## 88. 極性分析\n",
        "\n",
        "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
        "\n",
        "- \"The movie was full of incomprehensibilities.\"\n",
        "- \"The movie was full of fun.\"\n",
        "- \"The movie was full of excitement.\"\n",
        "- \"The movie was full of crap.\"\n",
        "- \"The movie was full of rubbish.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EruVW7JaapIv"
      },
      "source": [
        "## 89. アーキテクチャの変更\n",
        "\n",
        "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}