{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ADPET68xjlzr",
        "tags": []
      },
      "source": [
        "# 第10章: 事前学習済み言語モデル（GPT型）\n",
        "\n",
        "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IxpqfMR3tzYa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "C1xKmMckti92",
        "tags": []
      },
      "source": [
        "## 90. 次単語予測\n",
        "\n",
        "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "48EIUcopyPw9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "EPTFZDNEWQBC",
        "outputId": "00502619-7ba0-4072-ec56-9598f44b08b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "print(f\"inputs_ids shape:{encoded_input['input_ids'].shape}\\nattention_mask shape:{encoded_input['attention_mask'].shape}\\n\")\n",
        "print(encoded_input)\n",
        "token = tokenizer.decode(encoded_input['input_ids'][0])\n",
        "print(f\"tokens: {token}\\n\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**encoded_input)\n",
        "\n",
        "\n",
        "logits = output.logits\n",
        "print(logits.shape) #shape: [B, L, V]\n",
        "\n",
        "last_token_logits = logits[0, -1, :]  #これで一番最後のトークンの次のロジット\n",
        "print(last_token_logits.shape)\n",
        "\n",
        "last_token_logits_prob = F.softmax(last_token_logits, dim=0)\n",
        "\n",
        "topk = torch.topk(last_token_logits_prob, k=10)\n",
        "\n",
        "for idx, score in zip(topk.indices, topk.values):\n",
        "    print(f\"{tokenizer.decode(idx)}: {score.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8wNaYzrusro",
        "outputId": "55b49263-7d3a-4306-d34b-5ffe32577a05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_ids shape:torch.Size([1, 5])\n",
            "attention_mask shape:torch.Size([1, 5])\n",
            "\n",
            "{'input_ids': tensor([[ 464, 3807,  373, 1336,  286]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
            "tokens: The movie was full of\n",
            "\n",
            "torch.Size([1, 5, 50257])\n",
            "torch.Size([50257])\n",
            " jokes: 0.0219\n",
            " great: 0.0186\n",
            " laughs: 0.0115\n",
            " bad: 0.0109\n",
            " surprises: 0.0107\n",
            " references: 0.0105\n",
            " fun: 0.0100\n",
            " humor: 0.0074\n",
            " \": 0.0074\n",
            " the: 0.0067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "s1RhOldA0meh",
        "tags": []
      },
      "source": [
        "## 91. 続きのテキストの予測\n",
        "\n",
        "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#greedy\n",
        "import time\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  output = model.generate(encoded_input['input_ids'], max_length=15)\n",
        "  end_time = time.time()\n",
        "\n",
        "print(f\"time: {(end_time - start_time):.4f}\")\n",
        "print(output.shape) #shape: [B,L]\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False)) #確率が最も高いトークンを１つずつ順番に選んでいく->greedy decoding デフォはこれっぽい\n"
      ],
      "metadata": {
        "id": "Gx9IWBHmGHjN",
        "outputId": "921f50b1-7002-40d2-a175-0862989562f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 0.9133\n",
            "torch.Size([1, 15])\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#beam_serch\n",
        "import time\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  output = model.generate(encoded_input['input_ids'], max_length=15, num_beams=5)\n",
        "  end_time = time.time()\n",
        "\n",
        "print(f\"time: {(end_time - start_time):.3f}s\")\n",
        "print(output.shape) #shape: [B,L]\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False)) #ビームサーチは貪欲法と比較して処理時間がおそい．\n"
      ],
      "metadata": {
        "id": "_2bEWV6dpznj",
        "outputId": "dc72aac3-0373-4258-f236-279de0561f71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.820s\n",
            "torch.Size([1, 15])\n",
            "The movie was full of jokes and jokes and jokes and jokes and jokes and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topk\n",
        "import time\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  output = model.generate(\n",
        "      encoded_input['input_ids'],\n",
        "      max_length=15,\n",
        "      top_k=50,\n",
        "      top_p=0.9,\n",
        "      temperature=2.0)\n",
        "  end_time = time.time()\n",
        "\n",
        "print(f\"time: {(end_time - start_time):.3f}s\")\n",
        "print(output.shape) #shape: [B,L]\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False))"
      ],
      "metadata": {
        "id": "_fMBlNtwwstH",
        "outputId": "35f69629-f9e9-44bd-a1b5-13b2769fb491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 0.812s\n",
            "torch.Size([1, 15])\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "7ZFadg6B8VdA",
        "tags": []
      },
      "source": [
        "## 92. 予測されたテキストの確率を計算\n",
        "\n",
        "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "\n",
        "  output = model.generate(\n",
        "      input_ids,\n",
        "      max_length=15,\n",
        "      top_k=50,\n",
        "      top_p=1.0,\n",
        "      temperature=1.0,\n",
        "      return_dict_in_generate=True,\n",
        "      output_scores=True\n",
        "  )\n",
        "  end_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "generated_ids = output.sequences[0]\n",
        "print(f\"generated_ids shape: {generated_ids.shape}\")\n",
        "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "print(f\"time: {(end_time - start_time):.3f}s\")\n",
        "print(f\"\\nGenerated Text:\\n{generated_text}\\n\")\n",
        "\n",
        "print(len(output.scores)) #tuple: max_len - input_len (=generated_len)\n",
        "print(output.scores[0].shape) #shape: [1, vocab_size]\n",
        "\n",
        "for score in output.scores:\n",
        "  prob = F.softmax(score, dim=1)\n",
        "  vocab_ids = torch.argmax(score, dim=1) #shape: [batch_size, ]\n",
        "  tokens = tokenizer.decode(vocab_ids)\n",
        "  print(f\"{tokens}: P={prob[0, vocab_ids].item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31qsnFz5Aq7_",
        "outputId": "6c64cb94-b8c4-4d6b-f0ad-f0ab2124ef3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_ids shape: torch.Size([15])\n",
            "time: 1.279s\n",
            "\n",
            "Generated Text:\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n",
            "\n",
            "10\n",
            "torch.Size([1, 50257])\n",
            " jokes: P=0.022\n",
            " and: P=0.289\n",
            " jokes: P=0.099\n",
            " about: P=0.206\n",
            " how: P=0.100\n",
            " the: P=0.085\n",
            " movie: P=0.036\n",
            " was: P=0.296\n",
            " a: P=0.068\n",
            " joke: P=0.174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FvNCTMj6OegF",
        "tags": []
      },
      "source": [
        "## 93. パープレキシティ\n",
        "\n",
        "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
        "\n",
        "+ The movie was full of surprises\n",
        "+ The movies were full of surprises\n",
        "+ The movie were full of surprises\n",
        "+ The movies was full of surprises\n",
        "\n",
        "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "sentences = ['The movie was full of surprises',\n",
        "             'The movies were full of surprises',\n",
        "             'The movie were full of surprises',\n",
        "             'The movies was full of surprises']\n",
        "\n",
        "def cal_ppl(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs['input_ids']) #loss = -1/L Sigma log(wt|wt<)\n",
        "        loss = outputs.loss\n",
        "        ppl = math.exp(loss.item()) #tensorだったからitem()でfloat型に変換\n",
        "    return ppl\n",
        "\n",
        "def my_cal_ppl(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    ids = inputs['input_ids'][0]  # shape: [seq_len]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
        "    log_probs = F.log_softmax(logits, dim=2)  # softmaxしてlogとる\n",
        "\n",
        "    loss_sum = 0.0\n",
        "    token_num = len(ids) - 1\n",
        "\n",
        "    # t番目の出力は t+1 番目のトークンを予測している\n",
        "    for i in range(1, len(ids)):\n",
        "        true_token_id = ids[i].item()\n",
        "        log_prob = log_probs[0, i - 1, true_token_id].item()\n",
        "        loss_sum += log_prob\n",
        "\n",
        "    ppl = math.exp(-loss_sum / token_num)\n",
        "    return ppl\n",
        "\n",
        "ppl = [cal_ppl(sentence) for sentence in sentences]\n",
        "print(ppl)\n",
        "\n",
        "my_ppl = [my_cal_ppl(sentence) for sentence in sentences]\n",
        "print(my_ppl)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tDcbc-TZ1tc3",
        "outputId": "f8e669bd-1191-4f74-c388-e178ba8af15d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[99.35493255487346, 126.480708931546, 278.879620238215, 274.6586517692294]\n",
            "[99.35488614108807, 126.4806989425864, 278.8796867283058, 274.65871848078285]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-7fB-n9suYg"
      },
      "source": [
        "## 94. チャットテンプレート\n",
        "\n",
        "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"User: What do you call a sweet eaten after dinner? AI: \"\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=1.0,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== 応答 ===\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "DuEq6lLmAgPR",
        "outputId": "49623ccd-5582-4b20-d870-cdc43755d89d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 応答 ===\n",
            "User: What do you call a sweet eaten after dinner? AI: ~~~-\n",
            "\n",
            "Peng: \"What\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "PT-bk0XWIZ2E",
        "tags": []
      },
      "source": [
        "## 95. マルチターンのチャット\n",
        "\n",
        "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = response + \" User: Please give me the plural form of the word with its spelling in reverse order. AI:\"\n",
        "print(second_prompt)\n",
        "\n",
        "encoded_input = tokenizer(second_prompt, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=1.0,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "second_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== 応答 ===\")\n",
        "print(second_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kNRsICNEjGy",
        "outputId": "19f4dbb5-40fe-454a-c39e-11773c8247f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What do you call a sweet eaten after dinner? AI: ~~~-\n",
            "\n",
            "Peng: \"What User: Please give me the plural form of the word with its spelling in reverse order. AI:\n",
            "=== 応答 ===\n",
            "User: What do you call a sweet eaten after dinner? AI: ~~~-\n",
            "\n",
            "Peng: \"What User: Please give me the plural form of the word with its spelling in reverse order. AI: ~~~-\"\n",
            "\n",
            "Cyril\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "qH0YortL0afd",
        "tags": []
      },
      "source": [
        "## 96. プロンプトによる感情分析\n",
        "\n",
        "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "id": "jr9CFJKjLccR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep='\\t', )\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep='\\t', )\n",
        "\n",
        "dev_data = list(zip(dev_df['sentence'], dev_df['label'])) #list of tuple\n"
      ],
      "metadata": {
        "id": "rQExKVNiLgWw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "lqqFG2fRLqqJ",
        "outputId": "c93257b4-e16a-44cf-aafb-2be99ac43248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(text, model, tokenizer):\n",
        "  prompt = f\"Review: {text} Sentiment: \"\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "  next_token_logits = logits[0, -1, :] #次の単語のlogits\n",
        "\n",
        "  positive_token_id = tokenizer.encode(' positive')[0]\n",
        "  negative_token_id = tokenizer.encode(' negative')[0]\n",
        "\n",
        "  positive_logit = next_token_logits[positive_token_id]\n",
        "  negative_logit = next_token_logits[negative_token_id]\n",
        "\n",
        "  return 1 if positive_logit > negative_logit else 0"
      ],
      "metadata": {
        "id": "MAct4LJyNwNu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "for tuple in tqdm(dev_data):\n",
        "  text = tuple[0]\n",
        "  label = tuple[1]\n",
        "  pred = classify(text, model, tokenizer)\n",
        "  if pred == label:\n",
        "    correct += 1\n"
      ],
      "metadata": {
        "id": "JcJkeR0vS55t",
        "outputId": "8cb3536b-66e1-4c47-dd9f-2112e5556324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 872/872 [03:03<00:00,  4.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct/ len(dev_data)) #これで本当にいいのか..."
      ],
      "metadata": {
        "id": "WuBOjw4WUaJp",
        "outputId": "a01059e9-6faf-488f-c99d-11a168b1ffa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6605504587155964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "giA6FivrKaSf",
        "tags": []
      },
      "source": [
        "## 97. 埋め込みに基づく感情分析\n",
        "\n",
        "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = 'distilgpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "XM6hyF14Nbpb",
        "outputId": "04aec453-727f-4119-9d4c-01a2cc12e718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SST2Dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.data.iloc[index]['sentence']\n",
        "        label = int(self.data.iloc[index]['label'])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            truncation=True,#max_lengthよりも長い場合に自動的にトリミングするようにしてる\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0), #__getitem__()は１つのサンプルを返す関数だからバッチ次元を取り除いた方がいいっぽい\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 3. Datasetの作成\n",
        "train_dataset = SST2Dataset(train_df, tokenizer)\n",
        "dev_dataset = SST2Dataset(dev_df, tokenizer)"
      ],
      "metadata": {
        "id": "mxSA7bcPHT0E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex = train_dataset.__getitem__(0)\n",
        "ex"
      ],
      "metadata": {
        "id": "WYm6YKrhHw1w",
        "outputId": "db6938ba-0fdc-4e15-c2e2-9cfc9158173f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([24717,   649,  3200,   507,   422,   262, 21694,  4991,   220, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'label': tensor(0)}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "i = 0\n",
        "for data in train_dataloader:\n",
        "    print(data[\"input_ids\"].shape) #shape: [batch_size, max_length]\n",
        "    print(data['label'].shape) #shape:[batch_size]\n",
        "    print(data)\n",
        "    i = i+1\n",
        "    if i > 0:\n",
        "      break #確認してみた"
      ],
      "metadata": {
        "id": "lDlqVdpkIm0X",
        "outputId": "abc42f84-38b4-469d-d592-42b5a1ce7cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 128])\n",
            "torch.Size([16])\n",
            "{'input_ids': tensor([[ 5562, 14509,   494,  ..., 50256, 50256, 50256],\n",
            "        [ 9442,   287,  1242,  ..., 50256, 50256, 50256],\n",
            "        [  271,  1364, 36907,  ..., 50256, 50256, 50256],\n",
            "        ...,\n",
            "        [ 8117,   705,    82,  ..., 50256, 50256, 50256],\n",
            "        [   69, 15451,   220,  ..., 50256, 50256, 50256],\n",
            "        [  505,   561,   307,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'label': tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model, num_categories=2, mlp_hidden_size=256, dropout_rate=0.3, loss_function=None):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Linear(pretrained_model.config.hidden_size, mlp_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(mlp_hidden_size, num_categories)\n",
        "        )\n",
        "\n",
        "        self.loss_function = loss_function if loss_function is not None else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        hidden_state = outputs.hidden_states[-1]  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # attention_maskが1の最後の位置を取得（バッチ対応）\n",
        "        last_token_indices = attention_mask.sum(dim=1) - 1  # shape: (batch_size,)\n",
        "        pooled_output = hidden_state[torch.arange(hidden_state.size(0)), last_token_indices]\n",
        "\n",
        "        logits = self.classifier_head(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_function(logits, labels)\n",
        "\n",
        "        return loss, logits\n"
      ],
      "metadata": {
        "id": "eFpdR4h_YQRS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_with_gpt2 = SentimentClassifier(model)\n",
        "classifier_with_gpt2.to(device)"
      ],
      "metadata": {
        "id": "Vv6wn7fiPmpj",
        "outputId": "c48b9941-84a7-4378-ad95-08a59cdcff32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentimentClassifier(\n",
              "  (pretrained_model): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-5): 6 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D(nf=2304, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=768)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D(nf=3072, nx=768)\n",
              "            (c_proj): Conv1D(nf=768, nx=3072)\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              "  (classifier_head): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
              "  )\n",
              "  (loss_function): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(classifier_with_gpt2.parameters(), lr=1e-4)\n",
        "for epoch in range(1):\n",
        "    classifier_with_gpt2.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, logits = classifier_with_gpt2(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "INPdQuXELP6U",
        "outputId": "30fab8fe-46e5-4c66-95c2-244f13da1ea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4210/4210 [07:35<00:00,  9.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2356.2462, Accuracy: 0.7075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "classifier_with_gpt2.eval()\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dev_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        # forward実行\n",
        "        _, logits = classifier_with_gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "dev_acc = correct / total\n",
        "print(f\"Dev Accuracy: {dev_acc:.4f}\")"
      ],
      "metadata": {
        "id": "20UrSM5WVGP6",
        "outputId": "a9ad8371-b0f0-472c-8f0a-3bbc8d11e386",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [00:05<00:00,  9.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Accuracy: 0.7993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "UnREZD3nTWUr",
        "tags": []
      },
      "source": [
        "## 98. ファインチューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z1IpiO5sbXIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_token_id = tokenizer.encode(' positive')[0]\n",
        "negative_token_id = tokenizer.encode(' negative')[0]"
      ],
      "metadata": {
        "id": "z9dsfOVHWtfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2SST2Dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.positive_token_id = tokenizer.encode(' positive')[0]\n",
        "        self.negative_token_id = tokenizer.encode(' negative')[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.data.iloc[index]['sentence']\n",
        "        label = int(self.data.iloc[index]['label'])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        label_token_id = self.positive_token_id if label == 1 else self.negative_token_id\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label_token_id': torch.tensor(label_token_id, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "nApSOhtXcKuy"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentClassifier2(nn.Module):\n",
        "    def __init__(self, pretrained_model, loss_function=None):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.loss_function = loss_function if loss_function is not None else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        lengths = attention_mask.sum(dim=1) - 1  # (batch_size,)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        logits_last = logits[torch.arange(batch_size), lengths]  # (batch_size, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_function(logits_last, labels)\n",
        "\n",
        "        return loss, logits_last\n"
      ],
      "metadata": {
        "id": "d8O_bhAUWliU"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep='\\t', )\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep='\\t', )\n",
        "train_df['sentence'] = train_df['sentence'].apply(lambda x: f\"Review: {x} Sentiment: \")\n",
        "dev_df['sentence'] = dev_df['sentence'].apply(lambda x: f\"Review: {x} Sentiment: \") #プロンプトの加工"
      ],
      "metadata": {
        "id": "VJCgvMKnnj0S"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = 'distilgpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model_dx = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "E6ts07z3m7mT"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ダミー入力\n",
        "sentence = \"This is a test.\"\n",
        "inputs = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "# モデルを通す\n",
        "outputs = model_dx(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "outputs.logits.shape"
      ],
      "metadata": {
        "id": "jevdPJuUqZ3d",
        "outputId": "bde4e871-3b39-4a95-9989-d9d585ab1982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = GPT2SST2Dataset(train_df, tokenizer)\n",
        "dev_dataset = GPT2SST2Dataset(dev_df, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "Lz-koaFzndS1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex = train_dataset.__getitem__(0)\n",
        "ex"
      ],
      "metadata": {
        "id": "_bu-a6QPodH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_with_gpt2_dx = SentimentClassifier2(model_dx)\n",
        "classifier_with_gpt2_dx.to(device)"
      ],
      "metadata": {
        "id": "hdmoIiAFktL7",
        "outputId": "23bb61b7-4a7b-44e3-9e58-5c5c6bb68dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-a98939f23199>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier_with_gpt2_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentClassifier2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier_with_gpt2_dx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQ74wHACkdJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(classifier_with_gpt2.parameters(), lr=1e-4)\n",
        "for epoch in range(1):\n",
        "    classifier_with_gpt2.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label_token_id\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss, logits = classifier_with_gpt2(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "guoIwzYGh8pt",
        "outputId": "638d3219-4c2e-4762-b5eb-6068df066298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4210 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-10d2371cf51c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_with_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     new_grads.append(\n\u001b[0;32m--> 220\u001b[0;31m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     )\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "4f0St5Ce0l34",
        "tags": []
      },
      "source": [
        "## 99. 選好チューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}