{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "ADPET68xjlzr",
        "tags": []
      },
      "source": [
        "# 第10章: 事前学習済み言語モデル（GPT型）\n",
        "\n",
        "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IxpqfMR3tzYa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "C1xKmMckti92",
        "tags": []
      },
      "source": [
        "## 90. 次単語予測\n",
        "\n",
        "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "48EIUcopyPw9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "EPTFZDNEWQBC",
        "outputId": "63ae6f65-585b-4d6b-b8d8-308b1831c729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "print(f\"inputs_ids shape:{encoded_input['input_ids'].shape}\\nattention_mask shape:{encoded_input['attention_mask'].shape}\\n\")\n",
        "print(encoded_input)\n",
        "token = tokenizer.decode(encoded_input['input_ids'][0])\n",
        "print(f\"tokens: {token}\\n\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**encoded_input)\n",
        "\n",
        "\n",
        "logits = output.logits\n",
        "print(logits.shape) #shape: [B, L, V]\n",
        "\n",
        "last_token_logits = logits[0, -1, :]  #これで一番最後のトークンの次のロジット\n",
        "print(last_token_logits.shape)\n",
        "\n",
        "last_token_logits_prob = F.softmax(last_token_logits, dim=0)\n",
        "\n",
        "topk = torch.topk(last_token_logits_prob, k=10)\n",
        "\n",
        "for idx, score in zip(topk.indices, topk.values):\n",
        "    print(f\"{tokenizer.decode(idx)}: {score.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8wNaYzrusro",
        "outputId": "454e2a27-9437-47e3-d2e3-8d2a0b075364"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_ids shape:torch.Size([1, 5])\n",
            "attention_mask shape:torch.Size([1, 5])\n",
            "\n",
            "{'input_ids': tensor([[ 464, 3807,  373, 1336,  286]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
            "tokens: The movie was full of\n",
            "\n",
            "torch.Size([1, 5, 50257])\n",
            "torch.Size([50257])\n",
            " jokes: 0.0219\n",
            " great: 0.0186\n",
            " laughs: 0.0115\n",
            " bad: 0.0109\n",
            " surprises: 0.0107\n",
            " references: 0.0105\n",
            " fun: 0.0100\n",
            " humor: 0.0074\n",
            " \": 0.0074\n",
            " the: 0.0067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "s1RhOldA0meh",
        "tags": []
      },
      "source": [
        "## 91. 続きのテキストの予測\n",
        "\n",
        "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#greedy\n",
        "import time\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  output = model.generate(encoded_input['input_ids'], max_length=15)\n",
        "  end_time = time.time()\n",
        "\n",
        "print(f\"time: {(end_time - start_time):.4f}\")\n",
        "print(output.shape) #shape: [B,L]\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False)) #確率が最も高いトークンを１つずつ順番に選んでいく->greedy decoding デフォはこれっぽい\n"
      ],
      "metadata": {
        "id": "Gx9IWBHmGHjN",
        "outputId": "23fd3d21-b95b-4f98-9800-0a8d409c6ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.3865\n",
            "torch.Size([1, 15])\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#beam_serch\n",
        "import time\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  output = model.generate(encoded_input['input_ids'], max_length=15, num_beams=5)\n",
        "  end_time = time.time()\n",
        "\n",
        "print(f\"time: {(end_time - start_time):.3f}s\")\n",
        "print(output.shape) #shape: [B,L]\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False)) #ビームサーチは貪欲法と比較して処理時間がおそい．\n"
      ],
      "metadata": {
        "id": "_2bEWV6dpznj",
        "outputId": "e2633f11-fed9-494b-ad65-1e5a261cdcb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.906s\n",
            "torch.Size([1, 15])\n",
            "The movie was full of jokes and jokes and jokes and jokes and jokes and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topk\n",
        "import time\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "  output = model.generate(\n",
        "      encoded_input['input_ids'],\n",
        "      max_length=15,\n",
        "      top_k=50,\n",
        "      top_p=0.9,\n",
        "      temperature=2.0)\n",
        "  end_time = time.time()\n",
        "\n",
        "print(f\"time: {(end_time - start_time):.3f}s\")\n",
        "print(output.shape) #shape: [B,L]\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False))"
      ],
      "metadata": {
        "id": "_fMBlNtwwstH",
        "outputId": "2a2ca2da-4079-410e-c51a-21543796c9c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.174s\n",
            "torch.Size([1, 15])\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "7ZFadg6B8VdA",
        "tags": []
      },
      "source": [
        "## 92. 予測されたテキストの確率を計算\n",
        "\n",
        "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"The movie was full of\"\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  start_time = time.time()\n",
        "\n",
        "  output = model.generate(\n",
        "      input_ids,\n",
        "      max_length=15,\n",
        "      top_k=50,\n",
        "      top_p=1.0,\n",
        "      temperature=1.0,\n",
        "      return_dict_in_generate=True,\n",
        "      output_scores=True\n",
        "  )\n",
        "  end_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "generated_ids = output.sequences[0]\n",
        "print(f\"generated_ids shape: {generated_ids.shape}\")\n",
        "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "print(f\"time: {(end_time - start_time):.3f}s\")\n",
        "print(f\"\\nGenerated Text:\\n{generated_text}\\n\")\n",
        "\n",
        "print(len(output.scores)) #tuple: max_len - input_len (=generated_len)\n",
        "print(output.scores[0].shape) #shape: [1, vocab_size]\n",
        "\n",
        "for score in output.scores:\n",
        "  prob = F.softmax(score, dim=1)\n",
        "  vocab_ids = torch.argmax(score, dim=1) #shape: [batch_size, ]\n",
        "  tokens = tokenizer.decode(vocab_ids)\n",
        "  print(f\"{tokens}: P={prob[0, vocab_ids].item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31qsnFz5Aq7_",
        "outputId": "1200020b-c85d-4cf8-94f7-d858e92d0a69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_ids shape: torch.Size([15])\n",
            "time: 1.059s\n",
            "\n",
            "Generated Text:\n",
            "The movie was full of jokes and jokes about how the movie was a joke\n",
            "\n",
            "10\n",
            "torch.Size([1, 50257])\n",
            " jokes: P=0.022\n",
            " and: P=0.289\n",
            " jokes: P=0.099\n",
            " about: P=0.206\n",
            " how: P=0.100\n",
            " the: P=0.085\n",
            " movie: P=0.036\n",
            " was: P=0.296\n",
            " a: P=0.068\n",
            " joke: P=0.174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "FvNCTMj6OegF",
        "tags": []
      },
      "source": [
        "## 93. パープレキシティ\n",
        "\n",
        "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
        "\n",
        "+ The movie was full of surprises\n",
        "+ The movies were full of surprises\n",
        "+ The movie were full of surprises\n",
        "+ The movies was full of surprises\n",
        "\n",
        "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "sentences = ['The movie was full of surprises',\n",
        "             'The movies were full of surprises',\n",
        "             'The movie were full of surprises',\n",
        "             'The movies was full of surprises']\n",
        "\n",
        "def cal_ppl(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs['input_ids']) #loss = -1/L Sigma log(wt|wt<)\n",
        "        loss = outputs.loss\n",
        "        ppl = math.exp(loss.item()) #tensorだったからitem()でfloat型に変換\n",
        "    return ppl\n",
        "\n",
        "def my_cal_ppl(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    ids = inputs['input_ids'][0]  # shape: [seq_len]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits  # shape: [1, seq_len, vocab_size]\n",
        "    log_probs = F.log_softmax(logits, dim=2)  # softmaxしてlogとる\n",
        "\n",
        "    loss_sum = 0.0\n",
        "    token_num = len(ids) - 1\n",
        "\n",
        "    # t番目の出力は t+1 番目のトークンを予測している\n",
        "    for i in range(1, len(ids)):\n",
        "        true_token_id = ids[i].item()\n",
        "        log_prob = log_probs[0, i - 1, true_token_id].item()\n",
        "        loss_sum += log_prob\n",
        "\n",
        "    ppl = math.exp(-loss_sum / token_num)\n",
        "    return ppl\n",
        "\n",
        "ppl = [cal_ppl(sentence) for sentence in sentences]\n",
        "print(ppl)\n",
        "\n",
        "my_ppl = [my_cal_ppl(sentence) for sentence in sentences]\n",
        "print(my_ppl)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tDcbc-TZ1tc3",
        "outputId": "f2a62bfb-67a9-43e4-bed5-4bfcefc11ca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[99.35393766125713, 126.48173421763245, 278.87789150142044, 274.66100919346405]\n",
            "[99.3538966517192, 126.48176606948401, 278.8779110327618, 274.66096990289424]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-7fB-n9suYg"
      },
      "source": [
        "## 94. チャットテンプレート\n",
        "\n",
        "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"User: What do you call a sweet eaten after dinner? AI: \"\n",
        "encoded_input = tokenizer(prompt, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=1.0,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== 応答 ===\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "DuEq6lLmAgPR",
        "outputId": "8feac3ae-12aa-4365-de46-dd8230b7751d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 応答 ===\n",
            "User: What do you call a sweet eaten after dinner? AI: 俺が漓れするい\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "PT-bk0XWIZ2E",
        "tags": []
      },
      "source": [
        "## 95. マルチターンのチャット\n",
        "\n",
        "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = response + \" User: Please give me the plural form of the word with its spelling in reverse order. AI:\"\n",
        "print(second_prompt)\n",
        "\n",
        "encoded_input = tokenizer(second_prompt, return_tensors='pt')\n",
        "input_ids = encoded_input['input_ids']\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=1.0,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "second_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"=== 応答 ===\")\n",
        "print(second_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kNRsICNEjGy",
        "outputId": "1b7a5975-3f23-465b-aab1-97599522ad08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What do you call a sweet eaten after dinner? AI: 俺が漓れするい User: Please give me the plural form of the word with its spelling in reverse order. AI:\n",
            "=== 応答 ===\n",
            "User: What do you call a sweet eaten after dinner? AI: 俺が漓れするい User: Please give me the plural form of the word with its spelling in reverse order. AI:暮に観も歡さ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "qH0YortL0afd",
        "tags": []
      },
      "source": [
        "## 96. プロンプトによる感情分析\n",
        "\n",
        "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "id": "jr9CFJKjLccR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep='\\t', )\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep='\\t', )\n",
        "\n",
        "dev_data = list(zip(dev_df['sentence'], dev_df['label'])) #list of tuple\n"
      ],
      "metadata": {
        "id": "rQExKVNiLgWw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "lqqFG2fRLqqJ",
        "outputId": "9f2780ee-188c-4b7a-f3ca-64e42381bf8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(text, model, tokenizer):\n",
        "  prompt = f\"Review: {text} Sentiment: \"\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "  next_token_logits = logits[0, -1, :] #次の単語のlogits\n",
        "\n",
        "  positive_token_id = tokenizer.encode(' positive')[0]\n",
        "  negative_token_id = tokenizer.encode(' negative')[0]\n",
        "\n",
        "  positive_logit = next_token_logits[positive_token_id]\n",
        "  negative_logit = next_token_logits[negative_token_id]\n",
        "\n",
        "  return 1 if positive_logit > negative_logit else 0"
      ],
      "metadata": {
        "id": "MAct4LJyNwNu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "for tuple in tqdm(dev_data):\n",
        "  text = tuple[0]\n",
        "  label = tuple[1]\n",
        "  pred = classify(text, model, tokenizer)\n",
        "  if pred == label:\n",
        "    correct += 1\n"
      ],
      "metadata": {
        "id": "JcJkeR0vS55t",
        "outputId": "9e4bf322-06b5-4b01-d67a-a60c5cb08358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 872/872 [03:49<00:00,  3.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct/ len(dev_data)) #これで本当にいいのか..."
      ],
      "metadata": {
        "id": "WuBOjw4WUaJp",
        "outputId": "a95602bc-e6f6-45dd-9391-875c119ef2f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6605504587155964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "text = \"hoge hoge\"\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "print(inputs)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs, output_hidden_states=True)\n",
        "  hidden_state = outputs.hidden_states[-1]\n",
        "  print(hidden_state.shape)\n"
      ],
      "metadata": {
        "id": "BqtpcQjnBMHi",
        "outputId": "045bf023-7fb1-451c-bec7-ebd7e4ec2176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[8873,  469, 8169,  469]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
            "torch.Size([1, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "giA6FivrKaSf",
        "tags": []
      },
      "source": [
        "## 97. 埋め込みに基づく感情分析\n",
        "\n",
        "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "UnREZD3nTWUr",
        "tags": []
      },
      "source": [
        "## 98. ファインチューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "4f0St5Ce0l34",
        "tags": []
      },
      "source": [
        "## 99. 選好チューニング\n",
        "\n",
        "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}